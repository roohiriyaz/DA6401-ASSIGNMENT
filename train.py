# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AE7w2u7hto4vBT6VpZ0p2t_u4JD1QZru
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from keras.datasets import fashion_mnist
import wandb
from sklearn.model_selection import train_test_split
wandb.login()

wandb.init(project="DA6401-ASSIGNMENT-1", entity="roohiparveen")
 (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()
fashion_mnist_labels = {
    0: 'T-shirt', 1: 'Trouser', 2: 'Pullover', 3: 'Dress', 4: 'Coat', 5: 'Sandal',
    6: 'Shirt', 7: 'Sneaker', 8: 'Bag', 9: 'Ankle Boot'
}
def get_class_label(class_index):
  return fashion_mnist_labels.get(class_index,'unknown')

samples = []
for label in range(10):
  idx = np.where(y_train == label)[0][0]
  samples.append((X_train[idx], get_class_label(label)))

wandb_images = []
for img, label in samples:
  wandb_images.append(wandb.Image(img, caption=label))

wandb.log({'DA6401-ASSIGNMENT-1':wandb_images})

wandb.finish()

wandb.init(
    project = 'DA6401-ASSIGNMENT-1',
    entity = 'roohiparveen',
    config={
        'input_size': 784,
        'hidden_sizes': [128,64],
        'output_size': 10,
        'activation': 'ReLU',
        'weight_init': 'random',
        'epochs': 10,
        'learning_rate': 0.01,
        "optimizer": "adam",
        "momentum": 0.9,
        "beta": 0.9,
        "beta1": 0.9,
        "beta2": 0.999,
        "epsilon": 1e-8
    }
)

(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()
X_train = X_train.reshape(X_train.shape[0],-1)/255.0
X_test = X_test.reshape(X_test.shape[0],-1)/255.0

def one_hot_encode(y, num_classes=10):
  return np.eye(num_classes)[y]

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state = 42)

y_train_one_hot = one_hot_encode(y_train)
y_val_one_hot = one_hot_encode(y_val)
y_test_one_hot = one_hot_encode(y_test)

"""original code"""

# Load Data
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()
X_train, X_test = X_train.reshape(-1, 784) / 255.0, X_test.reshape(-1, 784) / 255.0

# One-hot encoding for labels
def one_hot_encode(y, num_classes=10):
    return np.eye(num_classes)[y]

y_train, y_test = one_hot_encode(y_train), one_hot_encode(y_test)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)

# Define the Feedforward Neural Network
class FeedforwardNeuralNetwork:
    def __init__(self, input_size, hidden_layers, hidden_size, output_size, activation, weight_init, learning_rate, optimizer, weight_decay):
        self.input_size = input_size
        self.hidden_layers = hidden_layers
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.activation = activation
        self.learning_rate = learning_rate
        self.optimizer = optimizer
        self.weight_decay = weight_decay

        # Initialize weights and biases
        self.weights, self.biases = self.initialize_weights(weight_init)

        # Optimizer-specific parameters
        self.velocities_w = [np.zeros_like(w) for w in self.weights]
        self.velocities_b = [np.zeros_like(b) for b in self.biases]

        self.m_w = [np.zeros_like(w) for w in self.weights]  # First moment vector (Adam)
        self.m_b = [np.zeros_like(b) for b in self.biases]

        self.v_w = [np.zeros_like(w) for w in self.weights]  # Second moment vector (Adam/RMSprop)
        self.v_b = [np.zeros_like(b) for b in self.biases]

        self.t = 1  # Time step for Adam

    def initialize_weights(self, weight_init):
        np.random.seed(42)
        weights, biases = [], []
        layer_sizes = [self.input_size] + [self.hidden_size] * self.hidden_layers + [self.output_size]

        for i in range(len(layer_sizes) - 1):
            if weight_init == "random":
                w = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(1 / layer_sizes[i])
            elif weight_init == "Xavier":
                w = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2 / (layer_sizes[i] + layer_sizes[i + 1]))
            b = np.zeros((1, layer_sizes[i + 1]))
            weights.append(w)
            biases.append(b)
        return weights, biases

    def activation_function(self, x, derivative=False):
        if self.activation == "sigmoid":
            sig = 1 / (1 + np.exp(-x))
            return sig * (1 - sig) if derivative else sig
        elif self.activation == "tanh":
            return 1 - np.tanh(x) ** 2 if derivative else np.tanh(x)
        elif self.activation == "ReLU":
            return (x > 0).astype(float) if derivative else np.maximum(0, x)
        return x

    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)

    def cross_entropy_loss(self, y_pred, y_true):
        return -np.mean(np.sum(y_true * np.log(y_pred + 1e-8), axis=1))

    def forward_pass(self, X):
        activations, pre_activations = [X], []
        for i in range(len(self.weights) - 1):
            X = np.dot(X, self.weights[i]) + self.biases[i]
            pre_activations.append(X)
            X = self.activation_function(X)
            activations.append(X)
        X = np.dot(X, self.weights[-1]) + self.biases[-1]
        pre_activations.append(X)
        X = self.softmax(X)
        activations.append(X)
        return activations, pre_activations

    def backward_pass(self, X, y_true, activations, pre_activations):
        gradients_w, gradients_b = [None] * len(self.weights), [None] * len(self.biases)
        delta = activations[-1] - y_true

        gradients_w[-1] = np.dot(activations[-2].T, delta) / X.shape[0]
        gradients_b[-1] = np.mean(delta, axis=0, keepdims=True)

        for i in range(len(self.weights) - 2, -1, -1):
            delta = np.dot(delta, self.weights[i + 1].T) * self.activation_function(pre_activations[i], derivative=True)
            gradients_w[i] = np.dot(activations[i].T, delta) / X.shape[0]
            gradients_b[i] = np.mean(delta, axis=0, keepdims=True)

        return gradients_w, gradients_b

    def update_weights(self, gradients_w, gradients_b):
        for i in range(len(self.weights)):
            if self.optimizer == "sgd":
                self.weights[i] -= self.learning_rate * gradients_w[i]
                self.biases[i] -= self.learning_rate * gradients_b[i]
            elif self.optimizer == "momentum":
                self.velocities_w[i] = 0.9 * self.velocities_w[i] - self.learning_rate * gradients_w[i]
                self.weights[i] += self.velocities_w[i]
                self.biases[i] -= self.learning_rate * gradients_b[i]
            elif self.optimizer == "nag":
                lookahead_weights = self.weights[i] + 0.9 * self.velocities_w[i]
                delta = np.dot(lookahead_weights.T, gradients_w[i])
                self.velocities_w[i] = 0.9 * self.velocities_w[i] - self.learning_rate * delta
                self.weights[i] += self.velocities_w[i]
                self.biases[i] -= self.learning_rate * gradients_b[i]
            elif self.optimizer == "rmsprop":
                self.v_w[i] = 0.9 * self.v_w[i] + 0.1 * (gradients_w[i] ** 2)
                self.weights[i] -= (self.learning_rate / (np.sqrt(self.v_w[i]) + 1e-8)) * gradients_w[i]
                self.biases[i] -= self.learning_rate * gradients_b[i]

    def train(self, X_train, y_train, X_val, y_val, epochs):
        for epoch in range(epochs):
            activations, pre_activations = self.forward_pass(X_train)
            loss = self.cross_entropy_loss(activations[-1], y_train)

            y_pred_train = np.argmax(activations[-1], axis=1)
            y_true_train = np.argmax(y_train, axis=1)
            train_acc = np.mean(y_pred_train == y_true_train)

            gradients_w, gradients_b = self.backward_pass(X_train, y_train, activations, pre_activations)
            self.update_weights(gradients_w, gradients_b)

            val_activations, _ = self.forward_pass(X_val)
            val_loss = self.cross_entropy_loss(val_activations[-1], y_val)

            y_pred_val = np.argmax(val_activations[-1], axis=1)
            y_true_val = np.argmax(y_val, axis=1)
            val_acc = np.mean(y_pred_val == y_true_val)

            wandb.log({"epoch": epoch + 1, "train_loss": loss, "val_loss": val_loss, "train_acc": train_acc, "val_acc": val_acc})

            print(f"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f} - Acc: {train_acc:.4f} - Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.4f}")

def train_sweep():
    with wandb.init() as run:
        config = wandb.config  # Load hyperparameters from the sweep

        # Set a meaningful run name in WandB
        run.name = f"hl_{config.hidden_layers}_bs_{config.batch_size}_ac_{config.activation}_opt_{config.optimizer}"

        # Initialize the neural network with sweep parameters
        nn = FeedforwardNeuralNetwork(
            input_size=784,
            hidden_layers=config.hidden_layers,
            hidden_size=config.hidden_size,
            output_size=10,
            activation=config.activation,
            weight_init=config.weight_init,
            learning_rate=config.learning_rate,
            optimizer=config.optimizer,
            weight_decay=config.weight_decay
        )

        # Train the model using the selected hyperparameters
        nn.train(X_train, y_train, X_val, y_val, epochs=config.epochs)

sweep_id = wandb.sweep(sweep_config, project="DA6401-ASSIGNMENT-1")
wandb.agent(sweep_id, function=train_sweep, count=10)

import wandb
import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Initialize WandB for logging
with wandb.init(project="DA6401-ASSIGNMENT-1", name="Best_Model_Evaluation"):

    # Retrieve best hyperparameters from WandB
    best_run = wandb.Api().sweep("DA6401-ASSIGNMENT-1/1prrlv8u").best_run()
    best_config = best_run.config
    print("Best Model Hyperparameters:", best_config)

    # Create the best model
    best_model = FeedforwardNeuralNetwork(
        input_size=784,
        hidden_layers=best_config["hidden_layers"],
        hidden_size=best_config["hidden_size"],
        output_size=10,
        activation=best_config["activation"],
        weight_init=best_config["weight_init"],
        learning_rate=best_config["learning_rate"],
        optimizer=best_config["optimizer"],
        weight_decay=best_config["weight_decay"]
    )

    # Train with the full dataset
    best_model.train(X_train, y_train, X_val, y_val, epochs=best_config["epochs"])

    # Forward pass on test set
    test_activations, _ = best_model.forward_pass(X_test)
    y_pred_test = np.argmax(test_activations[-1], axis=1)
    y_true_test = np.argmax(y_test, axis=1)

    # Compute test accuracy
    test_accuracy = np.mean(y_pred_test == y_true_test)
    print(f"Test Accuracy: {test_accuracy:.4f}")

    # Log test accuracy to WandB
    wandb.log({"test_accuracy": test_accuracy})

    # Compute Confusion Matrix
    cm = confusion_matrix(y_true_test, y_pred_test)

    # Plot Creative Confusion Matrix
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt="d", cmap="coolwarm", linewidths=1, linecolor="black")

    # Customize Labels
    plt.xlabel("Predicted Label", fontsize=12)
    plt.ylabel("True Label", fontsize=12)
    plt.title("Creative Confusion Matrix for Fashion-MNIST", fontsize=14)

    # Save and log the confusion matrix
    plt.savefig("confusion_matrix.png")
    wandb.log({"confusion_matrix": wandb.Image("confusion_matrix.png")})

    # Show the plot
    plt.show()

# Load Data
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()
X_train, X_test = X_train.reshape(-1, 784) / 255.0, X_test.reshape(-1, 784) / 255.0

# One-hot encoding for labels
def one_hot_encode(y, num_classes=10):
    return np.eye(num_classes)[y]

y_train, y_test = one_hot_encode(y_train), one_hot_encode(y_test)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)

# Define the Feedforward Neural Network
class FeedforwardNeuralNetwork:
    def __init__(self, input_size, hidden_layers, hidden_size, output_size, activation, weight_init, learning_rate, optimizer, weight_decay):
        self.input_size = input_size
        self.hidden_layers = hidden_layers
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.activation = activation
        self.learning_rate = learning_rate
        self.optimizer = optimizer
        self.weight_decay = weight_decay

        # Initialize weights and biases
        self.weights, self.biases = self.initialize_weights(weight_init)

        # Optimizer-specific parameters
        self.velocities_w = [np.zeros_like(w) for w in self.weights]
        self.velocities_b = [np.zeros_like(b) for b in self.biases]

        self.m_w = [np.zeros_like(w) for w in self.weights]  # First moment vector (Adam)
        self.m_b = [np.zeros_like(b) for b in self.biases]

        self.v_w = [np.zeros_like(w) for w in self.weights]  # Second moment vector (Adam/RMSprop)
        self.v_b = [np.zeros_like(b) for b in self.biases]

        self.t = 1  # Time step for Adam

    def initialize_weights(self, weight_init):
        np.random.seed(42)
        weights, biases = [], []
        layer_sizes = [self.input_size] + [self.hidden_size] * self.hidden_layers + [self.output_size]

        for i in range(len(layer_sizes) - 1):
            if weight_init == "random":
                w = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(1 / layer_sizes[i])
            elif weight_init == "Xavier":
                w = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2 / (layer_sizes[i] + layer_sizes[i + 1]))
            b = np.zeros((1, layer_sizes[i + 1]))
            weights.append(w)
            biases.append(b)
        return weights, biases

    def activation_function(self, x, derivative=False):
        if self.activation == "sigmoid":
            sig = 1 / (1 + np.exp(-x))
            return sig * (1 - sig) if derivative else sig
        elif self.activation == "tanh":
            return 1 - np.tanh(x) ** 2 if derivative else np.tanh(x)
        elif self.activation == "ReLU":
            return (x > 0).astype(float) if derivative else np.maximum(0, x)
        return x

    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)

    #def cross_entropy_loss(self, y_pred, y_true):
     #   return -np.mean(np.sum(y_true * np.log(y_pred + 1e-8), axis=1))

    def mse_loss(self, y_true, y_pred):
      return np.mean(np.sum((y_true - y_pred) ** 2, axis=1))

    def forward_pass(self, X):
        activations, pre_activations = [X], []
        for i in range(len(self.weights) - 1):
            X = np.dot(X, self.weights[i]) + self.biases[i]
            pre_activations.append(X)
            X = self.activation_function(X)
            activations.append(X)
        X = np.dot(X, self.weights[-1]) + self.biases[-1]
        pre_activations.append(X)
        X = self.softmax(X)
        activations.append(X)
        return activations, pre_activations

    def backward_pass(self, X, y_true, activations, pre_activations):
        gradients_w, gradients_b = [None] * len(self.weights), [None] * len(self.biases)
        delta = activations[-1] - y_true

        gradients_w[-1] = np.dot(activations[-2].T, delta) / X.shape[0]
        gradients_b[-1] = np.mean(delta, axis=0, keepdims=True)

        for i in range(len(self.weights) - 2, -1, -1):
            delta = np.dot(delta, self.weights[i + 1].T) * self.activation_function(pre_activations[i], derivative=True)
            gradients_w[i] = np.dot(activations[i].T, delta) / X.shape[0]
            gradients_b[i] = np.mean(delta, axis=0, keepdims=True)

        return gradients_w, gradients_b

    def update_weights(self, gradients_w, gradients_b):
        for i in range(len(self.weights)):
            if self.optimizer == "sgd":
                self.weights[i] -= self.learning_rate * gradients_w[i]
                self.biases[i] -= self.learning_rate * gradients_b[i]
            elif self.optimizer == "momentum":
                self.velocities_w[i] = 0.9 * self.velocities_w[i] - self.learning_rate * gradients_w[i]
                self.weights[i] += self.velocities_w[i]
                self.biases[i] -= self.learning_rate * gradients_b[i]
            elif self.optimizer == "nag":
                lookahead_weights = self.weights[i] + 0.9 * self.velocities_w[i]
                delta = np.dot(lookahead_weights.T, gradients_w[i])
                self.velocities_w[i] = 0.9 * self.velocities_w[i] - self.learning_rate * delta
                self.weights[i] += self.velocities_w[i]
                self.biases[i] -= self.learning_rate * gradients_b[i]
            elif self.optimizer == "rmsprop":
                self.v_w[i] = 0.9 * self.v_w[i] + 0.1 * (gradients_w[i] ** 2)
                self.weights[i] -= (self.learning_rate / (np.sqrt(self.v_w[i]) + 1e-8)) * gradients_w[i]
                self.biases[i] -= self.learning_rate * gradients_b[i]

    def train(self, X_train, y_train, X_val, y_val, epochs):
        for epoch in range(epochs):
            activations, pre_activations = self.forward_pass(X_train)
            loss = self.mse_loss(activations[-1], y_train)

            y_pred_train = np.argmax(activations[-1], axis=1)
            y_true_train = np.argmax(y_train, axis=1)
            train_acc = np.mean(y_pred_train == y_true_train)

            gradients_w, gradients_b = self.backward_pass(X_train, y_train, activations, pre_activations)
            self.update_weights(gradients_w, gradients_b)

            val_activations, _ = self.forward_pass(X_val)
            val_loss = self.mse_loss(val_activations[-1], y_val)

            y_pred_val = np.argmax(val_activations[-1], axis=1)
            y_true_val = np.argmax(y_val, axis=1)
            val_acc = np.mean(y_pred_val == y_true_val)

            wandb.log({"epoch": epoch + 1, "train_loss": loss, "val_loss": val_loss, "train_acc": train_acc, "val_acc": val_acc})

            print(f"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f} - Acc: {train_acc:.4f} - Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.4f}")
            print('mse_loss')

    def train_sweep():
        with wandb.init() as run:
            config = wandb.config  # Load hyperparameters from the sweep

            # Set a meaningful run name in WandB
            run.name = f"hl_{config.hidden_layers}_bs_{config.batch_size}_ac_{config.activation}_opt_{config.optimizer}"

            # Initialize the neural network with sweep parameters
            nn = FeedforwardNeuralNetwork(
                input_size=784,
                hidden_layers=config.hidden_layers,
                hidden_size=config.hidden_size,
                output_size=10,
                activation=config.activation,
                weight_init=config.weight_init,
                learning_rate=config.learning_rate,
                optimizer=config.optimizer,
                weight_decay=config.weight_decay
            )

            # Train the model using the selected hyperparameters
            nn.train(X_train, y_train, X_val, y_val, epochs=config.epochs)

sweep_id = wandb.sweep(sweep_config, project="DA6401-ASSIGNMENT-1")
wandb.agent(sweep_id, function=train_sweep, count=10)

